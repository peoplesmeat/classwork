% This template was created by Ben Mitchell
% for Dr. Sheppard's AI class, CS 335/435, spring 08.

% For those who want to learn LaTeX, this is a decent place to start:
% http://en.wikibooks.org/wiki/LaTeX
% Note that the proper pronunciation is "la tek", not "lay teks".
%
% There are lots of latex tutorials and primers online; just be careful with
% google images.

\documentclass[12pt,letterpaper]{article}

\usepackage{amsmath, amsthm, graphicx, multicol}
\usepackage{float}
\title{Konane: \\ Minimax Agents}
\author{Bill Davis \\ Johns Hopkins University}


\begin{document}
\maketitle

\subsection{Specify a hypothesis about expected performance} 
Based on the heuristic chosen for this Minimax agent I expect that agents which are configured to look deeper into the search tree will be more likely to win a Konane match against agents that do not look as deep. I would expect that agents that look at the same number of levels to each have a 50\% chance of winning a match. 

\subsection{Make a table of average number of nodes explored for each agent at each board size}

\begin{table}[h]
\begin{tabular}{ |c|c|c|c|c|c|c| }
\hline
Max. Depth  &   4x4 MiniMax & 4x4 $\alpha \beta$ &   6x6 MiniMax & 6x6 $\alpha \beta$ &   8x8 MiniMax & 8x8 $\alpha \beta$\\
\hline
1 & 16 & 13 & 54 & 59 & 172 & 164 \\
2 & 34 & 30 & 392 & 275 & 2193 & 1205 \\
3 & 59 & 43 & 1188 & 899 & 17590 & 6476 \\
4 & 143 & 87 & 21306 &599 & 233789 & 34733 \\
5 & 447 & 203 & 51746 & 15121 & 1504679 & 141188 \\
6 & 825 & 442 & 646304 & 64576 & - & 864165 \\
7 & 1832 & 739 & - & 247881 & - & - \\
8 & 3748 & 1571 & - & -  & - & - \\
\hline
\end{tabular}
\caption{The number of nodes explored for a given depth for both algorithms. }

\end{table}

\subsection{Number of nodes at each step during the game} 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Turn & MiniMax & $\alpha \beta$ \\
1 & 731 & 206 \\
2 & 632 & 247 \\
3 & 256 & 276 \\
4 & 132 & 71 \\ 
5 & 13 & 7 \\ 
6 & 1 & 0 \\
\hline
\end{tabular}
\caption{A 4x4 Game Board with a Max Depth for both agents of 7. This is an average over two games where each player is black once. }
\end{table}


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Turn & MiniMax & $\alpha \beta$ \\
1 & 494 & 154 \\ 
2 & 840 & 338 \\
3 & 3653 & 859 \\
4 & 11102 & 3780 \\ 
5 & 24830 & 7645 \\ 
6 & 32733 & 5136 \\
7 & 41418 & 3378 \\ 
8 & 25014 & 4262 \\ 
9 & 8519 & 1765 \\ 
10 & 7871 & 2391 \\
11 & 1123 & 432 \\ 
12 & 779 & 132 \\ 
13 & 151 & 51 \\
\hline
\end{tabular}
\caption{A 6x6 Game Board with a max search depth of 5}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Turn & MiniMax & $\alpha \beta$ \\
1 & 124 & 56 \\
2 & 164 & 117 \\
3 & 956 & 256 \\
4 & 3397 & 554 \\
5 & 2779 & 908 \\
6 & 6523 & 1137 \\
7 & 9297 & 1338\\
8 & 12808 & 1776\\
9  & 20284 & 1786\\
10 & 14548 & 3252\\
11 & 14769 & 4525\\
12 & 23673 & 4115\\
13 & 16350 & 3598\\
14 & 11060 & 2314\\
15 & 7627 & 1974\\
16 & 7526 & 2031\\
17 & 3841 & 1484\\
18 & 3881 & 1414\\
19 & 5194 & 1257\\
20 & 1712 & 841\\
21 & 700 & 344\\
22 & 245 & 104 \\
\hline
\end{tabular}
\caption{A 8x8 Game Board with a max search depth of 4}
\end{table}

\subsection{Discussion}
For the 4x4 board, the difference between the runtimes is almost negligible. Because there are few moves per game, and few pieces to keep track of, using either algorithm generates successful results, in a reasonable amount of time, even up to 15 moves. 

For the larger boards, game becomes longer and the number of pieces and possible moves greatly increases. In these games, Alpha-Beta pruning can greatly increase the performance of the Minimax algorithm. For example in the 8x8 board, on turn 12, Minimax without Alpha-Beta pruning examines 6 times more nodes. 

Both algorithms go through three distinct phases. First is the ramp-up phase. This is during the beginning of the game where there not many pieces have been removed. This cuts down on the number of possible moves. 

Once several moves have been made though a large number of possibilities open up. The second phase is during the middle of the game, where there are thousands of possible moves to examine, and the agent is incapable of seeing the end-game. This forces the agent to examine moves up until the maximum search depth. During this phase, Alpha-Beta pruning vastly outperforms normal Minimax. By cutting down on the number of moves examined, the performance of the algorithm can be significantly improved, and can search deeper during the same amount of time. 

When many pieces have been removed the number of possible moves winds down. In addition, once the agents get closer to end-game situations they both algorithms are able to shortcut their search for solutions without necessarily searching to their maximum search depth. While Alpha-Beta continues to outperform normal Minimax, the ratio of nodes explored decreases significantly from the middle phase of the game. 

\subsection{Describe the evaluation function}
The evaluation function I used only takes into account the number of moves possible at each board. If board being evaluated represents a win for the agent, the board is scored 30. If it is a loss, the board is scored -30. These two events occur when the agent is capable of seeing the end game. If the board still have moves available, the evaluation function scores the board equal to the number of moves possible by the agent minus the number of moves available to the opponent. 

\subsection{Describe how you came up with your evaluation function}
I arrived at this function after attempting to use several other factors including the number of pieces remaining on the board for each player, and the number of moves available to one or the other player. This later function, while good, was incapable of representing situations where the number of moves available to the agent might be high, but still lower than the opponent. BY taking into account both factors, the agent is able to move to positions that may have a lower number of moves available but is still higher than the opponent. By attempting to maximize this differential the agent is able to leave itself maximum flexibility, while constraining the number of moves available to the opponent. 

The winning and losing score of +- 30 was chosen because 30 is greater then the possible move differential during an 8x8 match. If larger boards were considered this value would likely have to be increased. 

One side effect of this evaluation function is that it pushes the game into situations where the agent has large number of moves to be considered. This can make the Minimax algorithm potentially more expensive by increasing the number of moves to be considered on the first step. 
\subsection{Conclusions}

Minimax and Minimax with Alpha-Beta pruning are both effective in playing the game Konane up to and including 8x8 boards. For smaller boards, like 4x4, both Algorithms are capable of seeing all possible moves from the start of the game. For larger boards, the number of levels capable of being searched is limited. 
Alpha-Beta pruning allows agents to search deeper into the search tree. To see whether searching more levels in the search tree is an effective game-playing strategy we can make a plot of probability outcomes when agents play each other. 


\begin{table}[h]
\centering

\begin{tabular}{c|c|c|c|c|c|}
\multicolumn{6}{c}{Probability of win for Player 1}  \\ 
\multicolumn{6}{c}{Depth of Search for Players 1/2}\\
 & 1 & 2 & 3 & 4 & 5 \\
\hline
1 &  0.45 &  0.60 &  0.45 &  0.15 &  0.25 \\
\hline
2 &  0.60 &  0.30 &  0.30 &  0.40 &  0.30 \\
\hline
3 &  0.95 &  0.55 &  0.55 &  0.35 &  0.25 \\
\hline
4 &  0.70 &  0.55 &  0.40 &  0.60 &  0.30 \\
\hline
5 &  0.95 &  0.65 &  0.60 &  0.65 &  0.40 \\
\hline
\end{tabular}
\caption{Probabilty of Player 1 Given search depths}
\end{table}

The above table was generated by having two agents play each other. Values from left to right represent player two increasing its search depth. Values moving top to bottom represent player one increasing its search depth. Each value describes the probability that player 1 will win the match. From this table we can that as values generally increase as they go down and decrease as they go right. This indicates that increased search depth increases the probability of winning. Furthermore, when the search depth is equal for both agents, the probability of winning is around 50\%. This validates the hypothesis that was put forward. 

\end{document}


